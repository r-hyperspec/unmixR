%\VignetteIndexEntry{Introduction to Hyperspectral Unmixing using unmixR}
%\VignetteKeywords{hyperspectral unmixing, vertex component analysis, VCA, NFINDR, N-FINDR, iterative constrained endmembers, ICE, spectroscopy, spectral mixing analysis}
%\VignetteEngine{knitr::knitr}

\documentclass[article, shortnames]{jss}

%% declarations for jss.cls %%%%%%%%%%%%%%%%

%% almost as usual
\author{Anton Belov\\Affiliation \And 
        Conor McManus\\Affiliation  \AND
        Claudia Beleites\\ Chemometrix GmbH  \And 
        Bryan A. Hanson\\DePauw University  \And 
        Simon Fuller\\Affiliation}

\title{Hyperspectral Unmixing Using the \proglang{R}\\Package \pkg{unmixR}}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Anton Belov, Conor McManus, Claudia Beleites, Bryan A. Hanson, Simon Fuller} %% comma-separated
\Plaintitle{Hyperspectral Unmixing Using the R Package unmixR} %% without formatting
\Shorttitle{\pkg{unmixR}: Hyperspectral Unmixing} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
Functions implementing the N-FINDR, Iterated Constrained Endmembers (ICE) and Vertex Component Analysis (VCA) algorithms which can recover pure component spectra and their respective concentrations from a hyperspectral data set.
}

\Keywords{hyperspectral unmixing, vertex component analysis, VCA, NFINDR, N-FINDR, iterative constrained endmembers, ICE, spectroscopy, \proglang{R}, spectral mixing analysis}

\Plainkeywords{hyperspectral unmixing, vertex component analysis, VCA, NFINDR, N-FINDR, iterative constrained endmembers, ICE, spectroscopy, R, spectral mixing analysis} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Claudia Beleites\\
  Chemometric Consulting and Chemometrix GmbH\\
  S{\"o}deler Weg 19, 61200 W{\"o}lfersheim, Germany\\
  E-mail: \email{Claudia.Beleites@chemometrix.eu}\\
  URL: \url{http://www.chemometrix.eu/index.html}\\
  \emph{and}\\
  Department of Spectroscopy and Imaging, Leibniz-Institute of Photonic Technology\\
  Albert-Einstein-Str. 9, 07745 Jena, Germany\\
  \\
  Bryan A. Hanson\\
  Dept. of Chemistry \& Biochemistry\\
  DePauw University\\
  602 S. College Ave.\\
  Greencastle IN 46135\\
  USA\\
  E-mail: \email{hanson@depauw.edu}\\
  URL: \url{http://academic.depauw.edu/~hanson/index.html}\\
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% end of declarations for jss.cls %%%%%%%%%%%%%%%%

% declarations by BH, CB next
\usepackage{amssymb}

\graphicspath{{./graphics/}}

\begin{document}

%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

% run a few things in the background for use later
<< SetUp, echo = FALSE, eval = TRUE, results = "hide" >>=
# R options & configuration:

rm(list = ls())
options(width =  50, show.signif.stars = FALSE)
desc <- packageDescription("unmixR")
vers <- paste("version ", desc$Version, sep = "")

suppressMessages(library("knitr")) # needed to access opts_chunk below
suppressMessages(library("unmixR"))
suppressMessages(library("hyperSpec"))
suppressMessages(library("microbenchmark"))


# Stuff specifically for knitr:

opts_chunk$set(out.width = "0.8\\textwidth", fig.align = "center", fig.width = 7, fig.height = 7, cache = FALSE)

@
\newpage

Notes for developers (to be removed later):
\begin{enumerate}
  \item This vignette is a work in progress and is based on unmixR \Sexpr{vers}.
  \item The JSS header and footer can be removed via documentclass[nojss]{jss}
  \item There are files in the vignettes folder which permit building the vignette by running make on the whole package, or just pointing to the directory and knitting (followed by typesetting).  The .bst and .cls files can eventually be removed as they are built-in to R.
\end{enumerate}

\section[Hyperspectral datasets]{Hyperspectral datasets}

Hyperspectral data are spectroscopic data collected in a spatial or temporal context.  Each sample observation is a spectrum collected at a particular location or time.  Each spectrum is thus accompanied by meta-data giving this information, which is needed to reconstruct images and for other types of analysis.  Figure~\ref{DataCube} illustrates the conceptual nature of a typical data set collected over a spatial ($x,y$) domain.


\begin{figure}
  \begin{center}
  \includegraphics[scale = 1.0]{DataCube.pdf}
  \caption{\label{DataCube}Unfolding of spectral data collected over an $x,y$ grid.}
  \end{center}
\end{figure}

Hyperspectral data sets are encountered in many disciplines, on scales from nanometers to hundreds of kilometers.  Typical applications include:

\begin{itemize}

  \item Biomedical imaging: a series of Raman spectra are collected as a microscopic piece of tissue is scanned in an $x,y$ plane.  Such studies are of interest for real time imaging of surgical tissue samples.  Other types of spectroscopy can be employed, for instance mass spectrometry (\cite{Bergner2013, Hedegaard2011}).

  \item Airborne imaging: A satellite, plane or drone flies a pattern over land and collects spectra in the visible, UV or NIR range.  Such studies have been used to map mineral formations, to study the quality of forest canopies, and for assessing the severity of wildfires (\cite{Green1998, Jetz2016, Veraverbeke2014}).

  \item Art history:  A painting is scanned to examine the underlying pigments and perhaps reveal the presence of additional hidden images (\cite{Dooley2013}).
  
  \item Quality control:  Pharmaceutical tablets are scanned using a hyperspectral camera during production to monitor coatings, active pharmaceutical ingredients and excipients (\cite{Kandpal2016, Franch-Lage2011}).
  
\end{itemize}

\section[Hyperspectral unmixing]{Hyperspectral unmixing}

Unmixing is the process of processing a data set such as those described above to extract the pure spectra that make up the data.  For instance, a mineral-rich landscape may have regions that consist of pure minerals and regions where minerals are mixed due to erosion and movement of materials.  Pure minerals have characteristic IR spectra.  Spectra collected as part of an airborne imaging study of the region would be expected to consist of many spectral mixtures as well as some pure or nearly pure spectra.

In applications which involve collecting data over a $x,y$ grid, the complete data set is often referred to as an image or "scene".  This scene is often said to be composed of "pixels" which would correspond to the sample spectra (in other words, a spectrum collected at a particular value of $x,y$).  The pure component spectra which are extracted by unmixing are typically referred to as "endmembers,"  but sometimes simply as "components."  These can be compared to databases or analyzed using the appropriate domain knowledge to determine their chemical identity.  Once the endmembers have been obtained, they can be used to calculate the abundances of each endmember at each point in the scene, resulting in an abundance map.  In the mineral-rich landscape example, this could be a map of a particular mineral of interest.  In a biomedical application, this map could identify cancerous from non-cancerous regions of a tissue sample.

\subsection[Algebraic description of the data]{Algebraic description of the data}
In the following discussion, lower-case bold letters represent vectors (matrices with a single row or column), e.g. $\mathbf{x}$.  The corresponding upper-case letter, $\mathbf{X}$,  is a matrix.  Since hyperspectral data sets are typically stored with samples in rows and wavelengths in columns, $\mathbf{x}$ would be a particular row of $\mathbf{X}$.

For hyperspectral data measured at $p$ wavelengths, and assuming $m$ endmembers are present, the data in a hyperspectral study can be represented in several ways.  A single sample observation is a spectrum at location (pixel) or time point.  It can be represented as linear mixing model

\begin{equation}
\mathbf{x}^{(1 \ \times \ p)} = \mathbf{a}^{(1 \ \times \ m)} \times \mathbf{E}^{(m \ \times \ p)} + \epsilon
\end{equation}

where $\mathbf{x}$ is the spectrum composed of $p$ wavelengths or channels, $\mathbf{a}$ gives the fraction of each endmember present at that particular pixel, and $\mathbf{E}$ is a matrix of the endmembers, the pure component spectra. $\epsilon$ is a noise term which is assumed to be a normally distributed.  $\mathbf{E}$ can be thought of as a library of reference spectra which is unknown before the unmixing process. In other words, a given spectrum is the sum of the product of each abundance times the corresponding reference spectrum, plus noise.

If we scale this up to a real data set containing $n$ samples, the equation above becomes

\begin{equation}
\mathbf{X}^{(n \ \times \ p)} = \mathbf{A}^{(n \ \times \ m)} \times \mathbf{E}^{(m \ \times \ p)} + \epsilon
\end{equation}

where $\mathbf{X}$ is a matrix of the raw data and $\mathbf{A}$ is a matrix giving the abundances of each endmember in each sample ($\mathbf{E}$ and $\epsilon$ as before).  Dropping the dimensional superscripts, the above can be written

\begin{equation}
\mathbf{X} = \mathbf{A}\mathbf{E} + \epsilon
\end{equation}

for simplicity.

If instead we scale down to a single wavelength in a single spectrum, the equation becomes

\begin{equation}
x_k = \mathbf{a} \times \mathbf{e}_k
\end{equation}

where $k$ is one of the $p$ wavelengths ($\mathbf{e}_k$ is one wavelength selected from the reference library $\mathbf{E}$).

The system is subject to several constraints.  The abundances must be positive, and they must sum to one.

\begin{equation}
0 \leqq a_i \leqq 1
\end{equation}

\begin{equation}
\sum_{i=1}^{m} a_i = 1
\end{equation}

where $i$ is one of the $m$ endmembers of the system.

\subsection[Data reduction]{Data reduction}

All hyperspectral unmixing algorithms begin with some sort of data reduction step.  There are several possibilities, but the most common is principal components analysis (PCA).  PCA is a widely used method which converts raw data into uncorrelated abstract components which can stand in for the data.  The advantage is that one doesn't need all the components to represent the data, because some or even many of them are merely noise, or redundant.  In the spectral context, this is equivalent to saying that some wavelengths are not informative, so we can discard them with little change to our analysis.  PCA is a dimensional reduction method because instead of having $p$ wavelengths, we now have many fewer abstract components to deal with.  Hence PCA not only removes noise but makes the problem computationally much faster by lowering the dimensionality of the problem.

After PCA (or a similar technique), the equation describing the system would be:

\begin{equation}
\mathbf{X} = \mathbf{A}\mathbf{E}
\end{equation}

where $\epsilon$ has effectively been eliminated.  However, at this stage $\mathbf{X}$ is no longer the original data, but rather the abstract components.  Its dimensions are still $n \ \times \ p$, but $p$ is no longer the number of wavelengths or channels.  Instead, it is the number of abstract components kept after the PCA procedure.  There may be a good argument to call $X$ and $p$ by some other symbol since they have changed substantially, but the habit in the literature is to ignore these changes from a symbolic point of view. From a practical point of view however, the dimension reduction that PCA provides makes the subsequent calculations computationally tractable.

\subsection[The role of the simplex]{The role of the simplex}

A key concept common to unmixing algorithms is that of a simplex.  A simplex is a container that holds the data points.  For two-dimensional data (i.e., measurements at two wavelengths), such as that illustrated in Figure~\ref{Simplex}, several kinds of containers can be imagined.  The "convex hull" is a container that is shrink-wrapped around the cloud of data points.  A simplex for two-dimensional data is a scalene triangle of the smallest possible size.

\begin{figure}
  \begin{center}
  \includegraphics[scale = 1.0]{Simplex2.pdf}
  \caption{\label{Simplex}Comparison of a convex hull to a 2-simplex for a data set collected at two wavelengths.}
  \end{center}
\end{figure}

The simplex is a useful concept in unmixing because its vertices are the endmembers.  In the two-dimensional example, the three vertices correspond theoretically to pure components.  Every other point inside the simplex can be thought of as a weighted average or mixture of each of the endmembers.

If one has spectra composed of $p$ wavelengths, the data exists as a point cloud in $p$-dimensions. For $p = 2$ as in Figure~\ref{Simplex}, the cloud of data can be captured in a 2-simplex or triangle.  For $p = 3$, a tetrahedron would be needed to contain the data.  For $p > 3$ there is no simple visual analog to the container, but it would have $p + 1$ edges.  Typical data sets might have hundreds or even thousands of wavelengths.  Simplices of this size are hard to imagine and would be computationally intractable regardless.  Recall however that prior to the unmixing process we reduce the data using PCA or a similar technique.  Thus, a data set of perhaps hundreds of wavelengths can be reduced to a much smaller dimensionality.  In practice, an appropriate value of $p$ is chosen (or perhaps we should say "guessed") by the researcher using the relevant domain knowledge.  The unmixing process then seeks to find these $p$ endmembers from which all the data can be reconstructed.

\section[Algorithms]{Algorithms}

A number of algorithms to extract endmembers have been described in the literature.  The chief difference between them is the method of finding the relevant simplex, which in turn identifies the endmembers.  In \pkg{unmixR}, there are variations of the N-FINDR, vertex component analysis (VCA) and iterated constrained endmembers (ICE) algorithms.  Once the endmembers are identified, the actual unmixing can be done to generate an abundance map.  These steps are discussed in the next sections.

\subsection[N-FINDR]{N-FINDR}

The N-FINDR method finds purest pixels in the scene, which are the vertices of the largest $p$-dimensional simplex that can be inscribed within the data.  An iterative inflation algorithm is used (\cite{Winter1999}).  This approach assumes that pure pixels are present in the scene, an assumption that doesn't always hold, but is not critical in practice.  In this algorithm $p$ pixels are chosen at random as a starting point, and the volume of the simplex is computed (via the absolute value of the determinant).  Then each of the chosen pixels is replaced sequentially by all of the pixels not currently chosen, checking the volume at each replacement.  If the volume increases, the new pixel is retained, otherwise the original pixel is retained.  This is the first pass.  This interim answer serves as a new starting point, and the process is repeated up to a specified number of iterations, or until the simplex volume cannot be inflated any further.  At this point, the vertices of the simplex are the purest pixels in the scene.

\textbf{Mention html vignette showing movies of the simplex identification process.}

\textbf{Figure 1 right panel is wrong or misleading.  Revise.}



\pkg{unmixR} includes several variants of this approach.  For testing purposes, the "Brute" method computes all possible simplices.  This will naturally be quite slow for all but trivial data sets.  Method "99" implements Winter's original algorithm (\cite{Winter1999} described above.  The iterative approach is necessary in the event that there are no pure pixels in the data set, as local maxima may be found rather than the global maximum.  Even so, the algorithm is not guaranteed to conduct an exhaustive search of all possible combinations of endmembers, and there is some sensitivity to the randomly chosen initial pixels (\cite{Plaza2005, Dowler2013}).

Dowler (\cite{Dowler2013}) describes two endmember extraction algorithms that are computationally more efficient.  Both address improvements in how the determinant of the simplex (i.e. the volume) is computed. \pkg{unmixR} provides implementations of Dowler's LDU-N-NFINDR algorithm as method "LDU" and his LDU-\emph{sequential} N-FINDR algorithm as method "SeqLDU".  In our benchmarking method "LDU" generally performs the best.

\subsection[Vertex component analysis]{Vertex component analysis}

\subsection[Iterated constrained endmembers]{Iterated constrained endmembers}

\subsection[Unmixing to generate abundance maps]{Unmixing to generate abundance maps}

\section[An example]{An example}

Much of the hyperspectral unmixing literature comes from the field of remote sensing where airborne platforms are employed to collect the data.  The mineral-rich landscape example above is not an abstraction.  A frequently used example is the airborne visible/infrared imaging spectrometer (AVIRIS) study of the cuprite region in the state of Nevada, USA.  This region is devoid of most vegetation and rich in geological formations (\cite{Green1998}).

The following results are based on unmixing a small portion of the cuprite data set.  The first figure shows two of the endmembers (careful work and comparison to ground-based geological studies suggest that there are at least 19 distinct mineral entities present).  Using the appropriate domain knowledge these endmembers can be identified.

\begin{center}
  \includegraphics[scale = 1.0]{endmembers.pdf}
\end{center}

The next figure is an abundance map of these endmembers.  Clearly if you were a prospector these maps would be a great guide to finding treasure!

\begin{center}
  \includegraphics[scale = 1.0]{map.pdf}
\end{center}

\section[Performance]{Performance}

Demonstrate bench-marking here.  Note: temporarily, there is a separate vignette to do the benchmarking.  Benchmarking will be way too slow for CRAN checking.

<< bench, cache = TRUE, eval = FALSE >>=

# This will be too slow for CRAN -- do as a separate task

library("microbenchmark")
res <- microbenchmark(
	Brute = nfindr(laser, p = 3, method = "Brute"),
	Winter = nfindr(laser, p = 3, method = "99"),
	LDU = nfindr(laser, p = 3, method = "LDU"),
	SeqLDU = nfindr(laser, p = 3, method = "SeqLDU")
	)
autoplot(res)
@

\section*{Acknowledgements}

We are grateful for support from the Google Summer of Code program to Conor McManus (2013) and Anton Belov (2016).

\bibliography{unmix}

\end{document}
